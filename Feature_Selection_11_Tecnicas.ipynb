#### 10 técnicas de seleção de variáveis/features mais usadas em Machine Learning com Python:

##### Recursive Feature Elimination (RFE) - elimina recursivamente as variáveis ​​menos importantes, com base no modelo treinado em cada iteração.
##### Matriz de Correlação - selecione as variáveis ​​que estão altamente correlacionadas com a variável alvo e elimine as que são altamente inter-relacionadas entre si.
##### SelectKBest - seleciona as variáveis ​​K melhores com base em uma estatística de estatística (como F-score ou informação mútua) e descarta o restante.
##### Feature Importance - determina a importância relativa de cada variável com base em um modelo treinado e seleciona as mais importantes.
##### Análise de Componentes Principais (PCA) - transforma as variáveis ​​originais em um conjunto de componentes principais não correlacionados, selecionando o que explicam a maior parte da variância dos dados.
##### Linear Discriminant Analysis (LDA) - semelhante ao PCA, mas otimizado para tarefas de classificação, selecionando as variáveis ​​que maximizam a separação entre as classes.
##### Ridge Regression - ajusta um modelo linear penalizando as variáveis ​​com coeficientes grandes, selecionando os que têm altitudes mais altas.
##### Lasso Regression - semelhante ao Ridge Regression, mas utiliza uma penalização diferente que tende a selecionar um subconjunto de variáveis ​​com coeficientes não nulos.
##### Elastic Net - combina as penalizações do Ridge e do Lasso Regression para obter um equilíbrio entre as vantagens de cada uma.
##### Forward Stepwise Selection - inicia com um modelo vazio e adiciona recursivamente uma variável que mais melhora o desempenho do modelo, até que nenhum ganho significativo seja obtido.


# Seleção de Features com Recursive Feature Elimination (RFE) 

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# gerando dados de exemplo
X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=0, random_state=1)

# criando um modelo de regressão logística
model = LogisticRegression()

# criando o seletor RFE com o modelo e o número de variáveis desejado
rfe = RFE(model, n_features_to_select=5)

# ajustando o seletor RFE aos dados
rfe.fit(X, y)

# imprimindo as variáveis selecionadas
print('Variáveis selecionadas: %s' % rfe.support_)




# Seleção de Features com Matriz de Correlação

import pandas as pd
import numpy as np

# gerando dados de exemplo
data = pd.DataFrame(np.random.rand(100, 5), columns=['var1', 'var2', 'var3', 'var4', 'var5'])

# calculando a matriz de correlação
corr_matrix = data.corr().abs()

# selecionando os pares de variáveis altamente correlacionadas
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]

# removendo as variáveis altamente correlacionadas
data = data.drop(data[to_drop], axis=1)

print(data.head())





# Seleção de Features com SelectKBest

from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest, f_classif

# carregando o conjunto de dados iris
iris = load_iris()

# selecionando as 2 melhores variáveis usando o teste ANOVA
X_new = SelectKBest(f_classif, k=2).fit_transform(iris.data, iris.target)

# imprimindo as duas melhores variáveis selecionadas
print(X_new[:5])




# Seleção de Features com Feature Importance


from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor

# carregando o conjunto de dados Boston Housing
boston = load_boston()

# criando o modelo de regressão aleatória com o conjunto de dados completo
model = RandomForestRegressor(random_state=42)
model.fit(boston.data, boston.target)

# imprimindo a importância de cada variável
for i, feature in enumerate(boston.feature_names):
    print("{}: {}".format(feature, model.feature_importances_[i]))




# Seleção de Features com Análise de Componentes Principais (PCA)

from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# carregando o conjunto de dados iris
iris = load_iris()

# realizando a análise de componentes principais com duas dimensões
pca = PCA(n_components=2)
X_new = pca.fit_transform(iris.data)

# imprimindo as duas dimensões das novas variáveis criadas
print(X_new[:5])




# Seleção de Features com Análise Discriminante Linear (LDA)

from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# carregando o conjunto de dados iris
iris = load_iris()

# realizando a análise discriminante linear (LDA)
lda = LinearDiscriminantAnalysis(n_components=2)
X_new = lda.fit_transform(iris.data, iris.target)

# imprimindo as novas variáveis (componentes discriminantes)
print(X_new[:5])




# Seleção de Features com Ridge Regression 
#### é uma técnica de seleção de variáveis que adiciona um termo de regularização à função objetivo da regressão linear, 
#### com o objetivo de reduzir a magnitude dos coeficientes menos importantes.


from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge

# carregando o conjunto de dados Boston Housing
boston = load_boston()

# realizando a regressão Ridge
ridge = Ridge(alpha=0.5)
ridge.fit(boston.data, boston.target)

# imprimindo os coeficientes das variáveis
print(ridge.coef_)




## Seleção de Features com Regressão de Lasso
#### A técnica Lasso Regression adiciona um termo de regularização à função objetivo da regressão linear, 
#### com o objetivo de forçar alguns dos coeficientes das variáveis menos importantes a zero.

from sklearn.datasets import load_boston
from sklearn.linear_model import Lasso

# carregando o conjunto de dados Boston Housing
boston = load_boston()

# realizando a regressão Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(boston.data, boston.target)

# imprimindo os coeficientes das variáveis
print(lasso.coef_)




## Seleção de Features com  ElasticNet
#### A técnica Elastic Net combina os efeitos da Ridge Regression e da Lasso Regression, 
#### adicionando uma combinação de termos de regularização à função objetivo da regressão linear.

from sklearn.datasets import load_boston
from sklearn.linear_model import ElasticNet

# carregando o conjunto de dados Boston Housing
boston = load_boston()

# realizando a regressão Elastic Net
enet = ElasticNet(alpha=0.1, l1_ratio=0.7)
enet.fit(boston.data, boston.target)

# imprimindo os coeficientes das variáveis
print(enet.coef_)


## Seleção de Features com  Forward Stepwise Selection
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import f_regression, SelectKBest

# carregando o dataset de exemplo
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = pd.Series(boston.target)

# definindo o número de variáveis a serem selecionadas
k = 5

# inicializando a lista de variáveis selecionadas
selected_vars = []

# iterando até selecionar o número de variáveis definido
while len(selected_vars) < k:
    remaining_vars = list(set(X.columns) - set(selected_vars))
    best_f = -np.inf
    best_var = None
    
    # iterando sobre as variáveis restantes e selecionando a melhor
    for var in remaining_vars:
        selected_vars.append(var)
        X_selected = X[selected_vars]
        _, f_pval = f_regression(X_selected, y)
        
        # mantendo a variável com o maior valor de F
        if f_pval[-1] > best_f:
            best_f = f_pval[-1]
            best_var = var
        
        selected_vars.remove(var)
    
    # adicionando a melhor variável à lista de variáveis selecionadas
    selected_vars.append(best_var)

# selecionando apenas as variáveis escolhidas
X_selected = X[selected_vars]

# treinando o modelo de regressão linear com as variáveis selecionadas
lr = LinearRegression().fit(X_selected, y)

# imprimindo o R² do modelo treinado
print("R² do modelo com as {} variáveis selecionadas: {:.3f}".format(k, lr.score(X_selected, y)))
